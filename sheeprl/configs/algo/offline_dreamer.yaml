# Offline Dreamer-V3 XL configuration

defaults:
  - default
  - /optim@world_model.optimizer: adam
  - /optim@actor.optimizer: adam
  - /optim@critic.optimizer: adam
  - _self_

name: offline_dreamer
gamma: 0.996996996996997
lmbda: 0.95
horizon: 15

# Training recipe
replay_ratio: 1
offline: True
offline_supervision: True
learning_starts: 1024
per_rank_pretrain_steps: 0
per_rank_sequence_length: ???
finetuning:
  freeze:
    encoder: False
    decoder: False
    actor: False
    critic: False
    world_model: False
    cbm_model: False

# Encoder and decoder keys
cnn_keys:
  decoder: ${algo.cnn_keys.encoder}
mlp_keys:
  decoder: ${algo.mlp_keys.encoder}

# Model related parameters
cnn_layer_norm:
  cls: sheeprl.models.models.LayerNormChannelLast
  kw:
    eps: 1e-3
mlp_layer_norm:
  cls: sheeprl.models.models.LayerNorm
  kw:
    eps: 1e-3
dense_units: 1024
mlp_layers: 5
dense_act: torch.nn.SiLU
cnn_act: torch.nn.SiLU
unimix: 0.01
hafner_initialization: True

# Compile related parameters
compile_dynamic_learning:
  disable: False
  fullgraph: False
compile_behaviour_learning:
  disable: False
  fullgraph: False
compile_compute_lambda_values:
  disable: False
  fullgraph: False

# World model
world_model:
  discrete_size: 32
  stochastic_size: 32
  kl_dynamic: 0.5
  kl_representation: 0.1
  kl_free_nats: 1.0
  kl_regularizer: 1.0
  continue_scale_factor: 1.0
  clip_gradients: 1000.0
  decoupled_rssm: False
  learnable_initial_recurrent_state: True

  # Encoder
  encoder:
    cnn_channels_multiplier: 96
    cnn_act: ${algo.cnn_act}
    dense_act: ${algo.dense_act}
    mlp_layers: ${algo.mlp_layers}
    cnn_layer_norm: ${algo.cnn_layer_norm}
    mlp_layer_norm: ${algo.mlp_layer_norm}
    dense_units: ${algo.dense_units}
    compile:
      disable: False
      fullgraph: True

  # Recurrent model
  recurrent_model:
    recurrent_state_size: 4096
    layer_norm: ${algo.mlp_layer_norm}
    dense_units: ${algo.dense_units}
    compile:
      disable: False
      fullgraph: True

  # Prior
  transition_model:
    hidden_size: 1024
    dense_act: ${algo.dense_act}
    layer_norm: ${algo.mlp_layer_norm}
    compile:
      disable: False
      fullgraph: True

  # Posterior
  representation_model:
    hidden_size: 1024
    dense_act: ${algo.dense_act}
    layer_norm: ${algo.mlp_layer_norm}
    compile:
      disable: False
      fullgraph: True

  # Decoder
  observation_model:
    cnn_channels_multiplier: ${algo.world_model.encoder.cnn_channels_multiplier}
    cnn_act: ${algo.cnn_act}
    dense_act: ${algo.dense_act}
    mlp_layers: ${algo.mlp_layers}
    cnn_layer_norm: ${algo.cnn_layer_norm}
    mlp_layer_norm: ${algo.mlp_layer_norm}
    dense_units: ${algo.dense_units}
    compile:
      disable: False
      fullgraph: True

  # Reward model
  reward_model:
    dense_act: ${algo.dense_act}
    mlp_layers: ${algo.mlp_layers}
    layer_norm: ${algo.mlp_layer_norm}
    dense_units: ${algo.dense_units}
    bins: 255
    compile:
      disable: False
      fullgraph: True

  # Discount model
  discount_model:
    learnable: True
    dense_act: ${algo.dense_act}
    mlp_layers: ${algo.mlp_layers}
    layer_norm: ${algo.mlp_layer_norm}
    dense_units: ${algo.dense_units}
    compile:
      disable: False
      fullgraph: True

  # CEM
  cbm_model:
    use_cbm: True
    n_concepts: 22
    emb_size: 16
    concept_type: ["bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin", "bin","bin"]
    concept_bins: [2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]
    ortho_reg: 1e-2
    concept_reg: 1e-2


  # World model optimizer
  optimizer:
    lr: 1e-4
    eps: 1e-8
    weight_decay: 0

# Actor
actor:
  cls: sheeprl.algos.offline_dreamer.agent.Actor
  ent_coef: 3e-4
  min_std: 0.1
  max_std: 1.0
  init_std: 2.0
  dense_act: ${algo.dense_act}
  mlp_layers: ${algo.mlp_layers}
  layer_norm: ${algo.mlp_layer_norm}
  dense_units: ${algo.dense_units}
  clip_gradients: 100.0
  unimix: ${algo.unimix}
  action_clip: 1.0
  compile:
    disable: False
    fullgraph: True

  # Disttributed percentile model (used to scale the values)
  moments:
    decay: 0.99
    max: 1.0
    percentile:
      low: 0.05
      high: 0.95

  # Actor optimizer
  optimizer:
    lr: 8e-5
    eps: 1e-5
    weight_decay: 0

# Critic
critic:
  dense_act: ${algo.dense_act}
  mlp_layers: ${algo.mlp_layers}
  layer_norm: ${algo.mlp_layer_norm}
  dense_units: ${algo.dense_units}
  per_rank_target_network_update_freq: 1
  tau: 0.02
  bins: 255
  clip_gradients: 100.0
  compile:
    disable: False
    fullgraph: True

  # Critic optimizer
  optimizer:
    lr: 8e-5
    eps: 1e-5
    weight_decay: 0

# Player agent (it interacts with the environment)
player:
  discrete_size: ${algo.world_model.discrete_size}
